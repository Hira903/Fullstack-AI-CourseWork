{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8809022",
   "metadata": {},
   "source": [
    "## LOADIND & SAMPLING DATA\n",
    "\n",
    "- The dataset was loaded using pandas for analysis.\n",
    "- The original dataset is large-scale; therefore, sampling was used to enable efficient iteration without\n",
    "compromising statistical validity.\n",
    "- Initially, a sample size of 50K was used. To assess representativeness, different sample sizes were\n",
    "tested. Increasing the sample size did not materially change the observed patterns, indicating that\n",
    "the sampled data adequately represents the underlying distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9a8334",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#loading file\n",
    "df = pd.read_csv('USA-Real-Estate.csv')\n",
    "\n",
    "#Sampling\n",
    "df_sample = df.sample(n=100000, random_state=42)\n",
    "\n",
    "#saving sampled file separately\n",
    "df_sample.to_csv(\"USA_data_sampled100K.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b715c3",
   "metadata": {},
   "source": [
    "## UNDERSTANDING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371216c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using pandas functions for understanding the data\n",
    "print(df_sample.head(10))\n",
    "print (\"---------------\")\n",
    "print(df_sample.tail(10))\n",
    "print (\"---------------\")\n",
    "print(\"shape of dataset ----> \\n\", df_sample.shape)\n",
    "print (\"---------------\")\n",
    "print(\"Information about dataset ----> \\n\", df_sample.info())\n",
    "print (\"---------------\")\n",
    "print(\"Statistical analysis ----> \\n\", df_sample.describe())\n",
    "print (\"---------------\")\n",
    "print (\"Columns in dataset ---> \\n\", df_sample.columns)\n",
    "print (\"---------------\")\n",
    "print (df_sample.dtypes)\n",
    "print (\"---------------\")\n",
    "\n",
    "#Checking missing value percentage\n",
    "missing_count = df_sample.isna().sum()\n",
    "missing_percentage = (missing_count/len(df_sample))*100\n",
    "\n",
    "df_missing = pd.DataFrame({\n",
    "    'Missing_Count' : missing_count,\n",
    "    'Missing_Percentage' : missing_percentage\n",
    "}).sort_values(by=\"Missing_Percentage\", ascending=False)\n",
    "\n",
    "print(df_missing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b09abe",
   "metadata": {},
   "source": [
    "*DATA UNDERSTANDING SUMMARY*\n",
    "\n",
    "In this step, the dataset was examined to understand its structure, feature types, summary statistics, and overall composition. Missing values and their percentages were analyzed across all columns, revealing that some features contain substantial missing data. This assessment helped identify potential data quality issues and informed decisions for exploratory data analysis and subsequent preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac54b89a",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "Exploratory data analysis was conducted to understand the distribution of the target variable, examine relationships between features and price, and identify patterns, trends, and outliers that may influence modeling decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb61be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Checking distributing of target column (price)\n",
    "sns.histplot(df_sample['price'], bins = 50, kde=True, color = 'skyblue')\n",
    "plt.title('Distribution of prices')\n",
    "plt.xlabel('price')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "#log-transformed price\n",
    "sns.histplot(np.log1p(df_sample['price']), bins = 50, kde=True, color = 'salmon')\n",
    "plt.title('Distribution of log (Prices + 1)')\n",
    "plt.xlabel('log (Prices + 1)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff5e8d9",
   "metadata": {},
   "source": [
    "*Finding:*\n",
    "\n",
    "The original price distribution was heavily right-skewed. Applying a log transformation reduced skewness and produced a more symmetric distribution, with most observations concentrated between 10 and 15 on the log scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dded17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Relationship between target column and numeric faetures\n",
    "\n",
    "# using regplot & lineplot both to study relationship of targret column with numeric feature columns\n",
    "\n",
    "''' As our target column is heavily skewed, its prefered to create its log column first for better analysis'''\n",
    "df_sample['log_price'] = np.log(df_sample[\"price\"] + 1) # +1 to avoid zero error\n",
    "\n",
    "#list of all numeric features\n",
    "numeric_features = ['house_size', 'bed', 'bath', 'acre_lot', 'brokered_by']\n",
    "\n",
    "# Scatterplot\n",
    "for feature in numeric_features:\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.regplot(x=df_sample[feature], y=df_sample['log_price'], color='purple')\n",
    "    plt.title(f'Price vs {feature}', fontsize=16)\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('log_price')\n",
    "    plt.show()\n",
    "\n",
    "# Lineplot\n",
    "for feature in numeric_features:\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.lineplot(x=df_sample[feature], y=df_sample['log_price'],color='red')\n",
    "    plt.title(f'Price vs {feature}', fontsize=16)\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('log_price')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f58965f",
   "metadata": {},
   "source": [
    "**Finding:**\n",
    "\n",
    "- house size vs log price  | Analysis: House size shows a clear upward trend with log price           \n",
    "- bed vs log price         | Analysis: Bedrooms show a mild positive effect on price                  \n",
    "- bath vs log price        | Analysis: Bathrooms show a stronger influence on price                   \n",
    "- acre lot vs log price    | Analysis: Acre lot shows minimal impact within observed range            \n",
    "- brokered_by vs log price | Analysis: Brokered_by shows no meaningful linear relationship with price  \n",
    "  \n",
    "*General Observation*\n",
    "\n",
    "Most numeric features are concentrated in lower ranges with a few outliers. House size and number of bathrooms show a clear upward trend with log price, bedrooms show a mild effect, while acre lot and brokered_by show minimal influence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7544e57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features VS Price\n",
    "\n",
    "#List of categorical columns\n",
    "cat_features = [\"status\", \"city\", \"state\"]\n",
    "\n",
    "#boxplot\n",
    "for feature in cat_features:\n",
    "    sns.boxplot(x=feature, y='log_price', data=df_sample, palette='Set3', hue=feature, legend=False)\n",
    "    plt.xticks(rotation=40)\n",
    "    plt.title(f'Price vs {feature}', fontsize=16)\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('log_price')\n",
    "    plt.show()\n",
    "\n",
    "#violinplot\n",
    "for feature in cat_features:\n",
    "    sns.violinplot(x=feature, y='log_price', data=df_sample, palette='Set3',  hue=feature, legend=False)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(f'Price vs {feature}', fontsize=16)\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('log_price')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7475e6bb",
   "metadata": {},
   "source": [
    "**Finding:**\n",
    "\n",
    "- Staus vs log price: \n",
    "        The box plot and violin plot show that “For Sale” properties have a median log price between 10 and 15 with a few high and low outliers, while “Ready to Build” properties have a slightly higher median around 13 to 15 with minimal outliers. This indicates that the property status has a modest impact on price.          \n",
    "- City vs log price:      \n",
    "        The box plot of price across cities is not clearly visible due to the very high number of unique cities. However, the range of log prices across cities is approximately 9 to 17, suggesting that city-level variation in price is relatively small within this dataset.                 \n",
    "- state vs log price:\n",
    "        The box plot of price by state shows median log prices in a similar range (roughly 8.5–15.5) across states such as North Carolina, Maryland, and Virginia, indicating that state-level differences exist but are not extremely large in this sample.\n",
    "\n",
    "*Violin Plots Note:*\n",
    "\n",
    "Violin plots for city and state could not be rendered due to high cardinality, but the price distributions inferred from the box plots are consistent with the expected spread observed in the dataset.\n",
    "  \n",
    "*GENERAL OBSERVATION*\n",
    "\n",
    "Overall, categorical features show modest variation in log price, with status having a slight effect, while city and state do not exhibit large differences in the majority of listings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a6e074",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap correlation\n",
    "\n",
    "numeric_for_corr = ['log_price', 'house_size', 'bed', 'bath', 'acre_lot', 'brokered_by']\n",
    "sns.heatmap(df_sample[numeric_for_corr].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap', fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae71ca3e",
   "metadata": {},
   "source": [
    "**Finding:**\n",
    "\n",
    "Correlation analysis indicates that house size and number of bathrooms are the most influential numeric features for predicting log price. Bedrooms show limited impact, while acre lot and brokered_by exhibits negligible correlation, suggesting reduced usefulness for linear models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1276a249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geographical price variation by state\n",
    "\n",
    "top_states = (df_sample.groupby('state')['price'].median().sort_values(ascending=False).head(10))\n",
    "sns.barplot(x=top_states.index, y=top_states.values, palette='viridis')\n",
    "plt.title('Top 10 States by Price')\n",
    "plt.ylabel('Median Price')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# geographical price variation by ZIP\n",
    "top_zips = (df_sample.groupby('zip_code')['price'].median().sort_values(ascending=False).head(30))\n",
    "print(top_zips)\n",
    "sns.barplot(x=top_zips.index.astype(str), y=top_zips.values, palette='magma')\n",
    "plt.title('Top 30 ZIP Codes by Median Price')\n",
    "plt.ylabel('Median Price')\n",
    "plt.show()\n",
    "\n",
    "# geographical price variation by city\n",
    "city_zips = (df_sample.groupby('city')['price'].median().sort_values(ascending=False).head(30))\n",
    "print(city_zips)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5cc396",
   "metadata": {},
   "source": [
    "**FINDINGS**\n",
    "\n",
    "**geographical price variation by state** \n",
    "\n",
    "Median house prices vary significantly by state, with the District of Columbia showing the highest median price (above $600,000), followed by Massachusetts and New York, indicating strong geographic influence on property values.\n",
    "\n",
    "**geographical price variation by ZIP**\n",
    "\n",
    "A small number of ZIP codes exhibit extremely high median prices (above $1.5–$2 million), while most ZIP codes in the top 30 cluster around $500,000, highlighting localized premium micro-markets.\n",
    "\n",
    "**geographical price variation by city**\n",
    "\n",
    "Median price by city and ZIP shows limited variation beyond the top few categories, while street is mostly unique. Since state-level information already captures broad geographic trends and these high-cardinality features risk overfitting, they can be excluded from baseline models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0136c7c7",
   "metadata": {},
   "source": [
    "## PREPROCESSING\n",
    "Based on insights from exploratory data analysis, the following preprocessing steps were applied to prepare the data for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd80f85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset again\n",
    "df_sample = pd.read_csv(\"USA_data_sampled100K.csv\")\n",
    "\n",
    "# Handling prev_sold_date column\n",
    "df_sample['ever_sold'] = df_sample['prev_sold_date'].notna().astype(int)\n",
    "df_sample.drop(columns=['prev_sold_date'], inplace=True)\n",
    "\n",
    "# Binary encoding 'status' column using mapping\n",
    "df_sample['status_encoded'] = df_sample['status'].map({'for_sale': 1,'ready_to_build': 0})\n",
    "df_sample.drop(columns=['status'], inplace=True)\n",
    "\n",
    "# Encoding 'state' using LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder #import library\n",
    "le = LabelEncoder()\n",
    "df_sample['state_encoded'] = le.fit_transform(df_sample['state'])\n",
    "df_sample.drop(columns=['state'], inplace=True)\n",
    "\n",
    "## Dealing high/low cardinality columns\n",
    "\n",
    "# Label encoding city column to check correlation with price (a rough check)\n",
    "\n",
    "le = LabelEncoder()\n",
    "df_sample['city_encoded'] = le.fit_transform(df_sample['city'].astype(str))\n",
    "# Check correlation with price\n",
    "print(df_sample[['city_encoded', 'price']].corr())\n",
    "\n",
    "'''               city_encoded     price\n",
    "city_encoded      1.000000  0.003763\n",
    "price             0.003763  1.000000\n",
    "\n",
    "these values are meaningless as numeric labels are arbitrary.\n",
    "'''\n",
    "\n",
    "#droping high cardinality columns\n",
    "df_sample.drop(columns=['city', 'zip_code', 'street', 'city_encoded'], inplace=True, errors='ignore')\n",
    "\n",
    "# using log_price column for our Final Modeling\n",
    "df_sample['log_price'] = np.log(df_sample[\"price\"] + 1)\n",
    "df_sample.drop(columns=['price'], inplace=True, errors='ignore')\n",
    "\n",
    "#handling missing values\n",
    "numeric_cols = ['bed', 'bath', 'acre_lot', 'house_size'] \n",
    "for col in numeric_cols:\n",
    "    df_sample[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "df_sample['brokered_by'].fillna(df['brokered_by'].mode()[0], inplace=True)\n",
    "\n",
    "# Drop rows with null log_price\n",
    "df_sample = df_sample.dropna(subset=['log_price'])\n",
    "\n",
    "# droping duplicated\n",
    "df_sample=df_sample.drop_duplicates()\n",
    "\n",
    "# checking for null values\n",
    "null = df_sample.isnull().sum()\n",
    "print(null)\n",
    "\n",
    "print()\n",
    "print(df_sample.head)\n",
    "print(df_sample.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c669335",
   "metadata": {},
   "source": [
    "**Observations and actions**\n",
    "- City, ZIP, street are high-cardinality columns in our dataset. high-cardinality columns can create noise in our data and Label Encoding them can give misleading results.\n",
    "- prev_sold_date cis not needed but the history of ever sold or never sold could be usefull so that column is encoded as 1/0.\n",
    "- city, street, ZIP code columns can be used for deep analysis but encoding them using LabelEncoder/OneHotEncoder would not be usefull due to high cardinality. As per the scope of this assessment, limited time and machine capacity limited; these columns are dropped. Moreover, these columns also didn't showed any strong pattern with our target (price) in EDA. However, in case of complusion by client; we can use HASHING or TARGET ENCODING to include these columns in our Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bef2e4d",
   "metadata": {},
   "source": [
    "\n",
    "## Exploratory Data Analysis After Preprocessing\n",
    "\n",
    "After handling missing values, encoding categorical variables, and transforming the target variable, the dataset is now clean and ready for modeling.  \n",
    "This step helps us visualize the relationships and distributions on the processed data, verify transformations, and ensure that preprocessing did not distort important patterns.  \n",
    "We will recreate key graphs and plots to confirm data integrity and feature-target relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e5e09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Relationship between target column and numeric faetures\n",
    "\n",
    "# using regplot & lineplot both to study relationship of targret column with numeric feature columns\n",
    "\n",
    "#list of all numeric features\n",
    "numeric_features = ['house_size', 'bed', 'bath', 'acre_lot', 'brokered_by']\n",
    "\n",
    "# Scatterplot\n",
    "for feature in numeric_features:\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.regplot(x=df_sample[feature], y=df_sample['log_price'], color='purple')\n",
    "    plt.title(f'Price vs {feature}', fontsize=16)\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('log_price')\n",
    "    plt.show()\n",
    "\n",
    "# Categorical features VS Price\n",
    "cat_features = [\"status_encoded\", \"state_encoded\"]\n",
    "#boxplot\n",
    "for feature in cat_features:\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.boxplot(x=feature, y='log_price', data=df_sample, palette='Set3', hue=feature, legend=False)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(f'Price vs {feature}', fontsize=16)\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('log_price')\n",
    "    plt.show()\n",
    "\n",
    "#Heatmap correlation\n",
    "numeric_for_corr = ['log_price', 'house_size', 'bed', 'bath', 'acre_lot', 'brokered_by']\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df_sample[numeric_for_corr].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap', fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e54b45",
   "metadata": {},
   "source": [
    "**Findings from EDA on Processed Data**\n",
    "\n",
    "After preprocessing, the key graphs and correlation analysis were recreated.  \n",
    "  \n",
    "- RegPlot and boxplots show similar patterns as before, with expected ranges and few outliers.  \n",
    "- Correlation heatmap is unchanged, confirming feature-target relationships.  \n",
    "\n",
    "✅ Preprocessing did not alter the inherent data patterns; the dataset is ready for feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c36f2ba",
   "metadata": {},
   "source": [
    "## FEATURE ENGINEERING\n",
    "\n",
    "After training the data, handling missing values, dropping unnecessary columns, and encoding categorical features, \n",
    "the dataset is now fully preprocessed and ready for feature engineering.  \n",
    "In this section, we will create new features and transform existing ones to improve model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a29045b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features\n",
    "x = df_sample[['brokered_by', 'bed', 'bath', 'acre_lot', 'house_size', 'ever_sold', 'status_encoded', 'state_encoded']]\n",
    "\n",
    "#target\n",
    "y = df_sample['log_price']\n",
    "\n",
    "#Spliting data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, train_size=.7, random_state=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37830431",
   "metadata": {},
   "source": [
    "## MODEL FITTING, TRAINING & TESTING\n",
    "\n",
    "Based on the insights from exploratory data analysis, the relationship between features and LogPrice appears suitable for linear modeling. \n",
    "In this section, we will train and evaluate different linear and tree-based regression algorithms to predict property prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060591bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model fitting\n",
    "from sklearn.linear_model import ElasticNet, Lasso, Ridge, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, root_mean_squared_error\n",
    "\n",
    "# Define models\n",
    "Models = {\"Linear Regression\": LinearRegression(),\n",
    "          \"Ridge Regression\": Ridge(alpha=1.0, random_state=32),\n",
    "          \"Lasso Regression\": Lasso(alpha=0.001, random_state=22),\n",
    "          \"ElasticNet\": ElasticNet(alpha=0.001, l1_ratio=0.5, random_state=22),\n",
    "          \"Random Forest Regressor\": RandomForestRegressor(n_estimators=100, random_state=22),\n",
    "          \"Gradiant Boosting Regressor\": GradientBoostingRegressor(n_estimators=100, random_state=22, learning_rate=0.1)}\n",
    "\n",
    "print(Models)\n",
    "\n",
    "#Creating an empty list\n",
    "results = []\n",
    "\n",
    "# training, predicting, evaluating\n",
    "for names, model in Models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Metrics for evaluation\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r_score =  r2_score(y_test, y_pred)\n",
    "    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "\n",
    "    # Back-transform to original price scale to get result in USD\n",
    "    mae_usd = mean_absolute_error(np.expm1(y_test), np.expm1(y_pred))\n",
    "    rmse_usd = np.sqrt(mean_squared_error(np.expm1(y_test), np.expm1(y_pred)))\n",
    "    \n",
    "    results.append({\n",
    "        'Model': names,\n",
    "        'MAE (log)': mae,\n",
    "        'RMSE (log)': rmse,\n",
    "        'R2': r_score,\n",
    "        'MAPE (%)': mape,\n",
    "        'MAE ($)': mae_usd,\n",
    "        'RMSE ($)': rmse_usd\n",
    "    })\n",
    "\n",
    "# Results in form of dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by='RMSE ($)')  # sort by best performing as RMSE($) is usually the most practical metric in pricing models\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40803135",
   "metadata": {},
   "source": [
    "*FINDINGS*\n",
    "\n",
    "The results indicate that tree-based models significantly outperform linear models for this dataset. \n",
    "Random Forest achieved the highest R² score (≈0.60) and the lowest MAE and RMSE, suggesting it captures non-linear relationships effectively.\n",
    "Linear models such as Linear Regression, Ridge, Lasso, and ElasticNet showed poor performance, confirming the weak linear relationships observed during EDA.\n",
    "\n",
    "*Evaluation Metric Note: MAPE*\n",
    "\n",
    "Mean Absolute Percentage Error (MAPE) was initially included to measure the average percentage deviation between actual and predicted prices. \n",
    "However, since the target variable was log-transformed and contained zero or near-zero values, MAPE resulted in infinite values due to division by zero.\n",
    "Therefore, MAPE was excluded from the final model comparison, and greater emphasis was placed on MAE, RMSE, and R², which provide more reliable evaluation for this dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1031a755",
   "metadata": {},
   "source": [
    "## Improving Model Accuracy\n",
    "\n",
    "Based on the initial model performance and evaluation metrics, further improvements can be achieved through hyperparameter tuning and the use of more advanced ensemble models.\n",
    "In this section, GridSearchCV is applied to optimize model parameters, and stronger boosting algorithms such as XGBoost and LightGBM are introduced to enhance predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931986e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# XGBoost Model\n",
    "xgb_model = XGBRegressor(n_estimators=300, learning_rate=0.05, max_depth=6, subsample=0.8, colsample_bytree=0.8, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# LGBMRegressor Model\n",
    "lgbm_model = LGBMRegressor(n_estimators=300, learning_rate=0.05, max_depth=-1, random_state=42)\n",
    "lgbm_model.fit(X_train, y_train)\n",
    "\n",
    "# Hyperparameter Tuning - Random Forest\n",
    "RF = RandomForestRegressor(random_state=42)\n",
    "\n",
    "Parameter_Grid_RF = {'n_estimators': [100, 200],\n",
    "            'max_depth': [None, 10],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'min_samples_leaf': [1, 2]}\n",
    "\n",
    "grid_RF = GridSearchCV(RF, Parameter_Grid_RF, cv=2, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "grid_RF.fit(X_train, y_train)\n",
    "best_RF = grid_RF.best_estimator_\n",
    "\n",
    "#Final Model Evaluation and Comparison\n",
    "models_advanced = {\n",
    "    'Tuned Random Forest': best_RF,\n",
    "    'XGBoost': xgb_model,\n",
    "    'LightGBM': lgbm_model}\n",
    "\n",
    "final_results = []\n",
    "\n",
    "for name, model in models_advanced.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse_log = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    rmse_usd = np.sqrt(mean_squared_error(np.expm1(y_test), np.expm1(y_pred)))\n",
    "    \n",
    "    final_results.append({\n",
    "        'Model': name,\n",
    "        'R2 Score': r2,\n",
    "        'RMSE (log)': rmse_log,\n",
    "        'RMSE ($)': rmse_usd})\n",
    "\n",
    "final_results_df = pd.DataFrame(final_results).sort_values(by='RMSE ($)')\n",
    "print(final_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1469bf22",
   "metadata": {},
   "source": [
    "## Final Model Findings\n",
    "\n",
    "After applying hyperparameter tuning and advanced ensemble models, the predictive performance of different models was evaluated using R² and RMSE metrics (both in log scale and actual price in USD).  \n",
    "\n",
    "**Observations:**\n",
    "- Among the tested models, **LightGBM and XGBoost** performed slightly better than Random Forest in terms of R² and RMSE.  \n",
    "- All three models have similar performance in terms of RMSE in actual price scale (≈2.16 million USD), indicating stable predictions.  \n",
    "- The final model can be chosen based on the **slight edge in accuracy, computational efficiency, or interpretability** depending on the use case.  \n",
    "\n",
    "These results demonstrate that ensemble learning and hyperparameter tuning can meaningfully improve predictive performance over baseline models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cae130",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The project demonstrates a full machine learning workflow for real estate price prediction:  \n",
    "1. Data loading and sampling  \n",
    "2. Data understanding and cleaning  \n",
    "3. EDA before and after preprocessing  \n",
    "4. Feature selection and engineering  \n",
    "5. Model training with linear and tree-based algorithms  \n",
    "6. Hyperparameter tuning to improve accuracy  \n",
    "\n",
    "The final model can reliably predict house prices based on the selected features, and the workflow can be extended for additional features or larger datasets in the future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
